{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Belief Networks.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egh3Vt5cLVd2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "2f058c5a-8180-41dd-d772-c5a8540a9a1f"
      },
      "source": [
        "!git clone https://github.com/lisa-lab/DeepLearningTutorials/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DeepLearningTutorials'...\n",
            "remote: Enumerating objects: 2, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 4722 (delta 1), reused 1 (delta 1), pack-reused 4720\u001b[K\n",
            "Receiving objects: 100% (4722/4722), 11.53 MiB | 13.72 MiB/s, done.\n",
            "Resolving deltas: 100% (2871/2871), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh3Nz72VL-y8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist_py3k.pkl.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rkat6D04MVkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1emNSl_OKqlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from __future__ import print_function, division\n",
        "import os\n",
        "os.chdir(\"DeepLearningTutorials/code\")\n",
        "import sys\n",
        "import timeit\n",
        "\n",
        "import numpy\n",
        "\n",
        "import theano\n",
        "import theano.tensor as T\n",
        "from theano.sandbox.rng_mrg import MRG_RandomStreams\n",
        "\n",
        "from logistic_sgd import LogisticRegression, load_data\n",
        "from mlp import HiddenLayer\n",
        "from rbm import RBM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_zrasdVKiOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# start-snippet-1\n",
        "class DBN(object):\n",
        "    \"\"\"Deep Belief Network\n",
        "\n",
        "    A deep belief network is obtained by stacking several RBMs on top of each\n",
        "    other. The hidden layer of the RBM at layer `i` becomes the input of the\n",
        "    RBM at layer `i+1`. The first layer RBM gets as input the input of the\n",
        "    network, and the hidden layer of the last RBM represents the output. When\n",
        "    used for classification, the DBN is treated as a MLP, by adding a logistic\n",
        "    regression layer on top.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, numpy_rng, theano_rng=None, n_ins=784,\n",
        "                 hidden_layers_sizes=[500, 500], n_outs=10):\n",
        "        \"\"\"This class is made to support a variable number of layers.\n",
        "\n",
        "        :type numpy_rng: numpy.random.RandomState\n",
        "        :param numpy_rng: numpy random number generator used to draw initial\n",
        "                    weights\n",
        "\n",
        "        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams\n",
        "        :param theano_rng: Theano random generator; if None is given one is\n",
        "                           generated based on a seed drawn from `rng`\n",
        "\n",
        "        :type n_ins: int\n",
        "        :param n_ins: dimension of the input to the DBN\n",
        "\n",
        "        :type hidden_layers_sizes: list of ints\n",
        "        :param hidden_layers_sizes: intermediate layers size, must contain\n",
        "                               at least one value\n",
        "\n",
        "        :type n_outs: int\n",
        "        :param n_outs: dimension of the output of the network\n",
        "        \"\"\"\n",
        "\n",
        "        self.sigmoid_layers = []\n",
        "        self.rbm_layers = []\n",
        "        self.params = []\n",
        "        self.n_layers = len(hidden_layers_sizes)\n",
        "\n",
        "        assert self.n_layers > 0\n",
        "\n",
        "        if not theano_rng:\n",
        "            theano_rng = MRG_RandomStreams(numpy_rng.randint(2 ** 30))\n",
        "\n",
        "        # allocate symbolic variables for the data\n",
        "\n",
        "        # the data is presented as rasterized images\n",
        "        self.x = T.matrix('x')\n",
        "\n",
        "        # the labels are presented as 1D vector of [int] labels\n",
        "        self.y = T.ivector('y')\n",
        "        # end-snippet-1\n",
        "        # The DBN is an MLP, for which all weights of intermediate\n",
        "        # layers are shared with a different RBM.  We will first\n",
        "        # construct the DBN as a deep multilayer perceptron, and when\n",
        "        # constructing each sigmoidal layer we also construct an RBM\n",
        "        # that shares weights with that layer. During pretraining we\n",
        "        # will train these RBMs (which will lead to chainging the\n",
        "        # weights of the MLP as well) During finetuning we will finish\n",
        "        # training the DBN by doing stochastic gradient descent on the\n",
        "        # MLP.\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "            # construct the sigmoidal layer\n",
        "\n",
        "            # the size of the input is either the number of hidden\n",
        "            # units of the layer below or the input size if we are on\n",
        "            # the first layer\n",
        "            if i == 0:\n",
        "                input_size = n_ins\n",
        "            else:\n",
        "                input_size = hidden_layers_sizes[i - 1]\n",
        "\n",
        "            # the input to this layer is either the activation of the\n",
        "            # hidden layer below or the input of the DBN if you are on\n",
        "            # the first layer\n",
        "            if i == 0:\n",
        "                layer_input = self.x\n",
        "            else:\n",
        "                layer_input = self.sigmoid_layers[-1].output\n",
        "\n",
        "            sigmoid_layer = HiddenLayer(rng=numpy_rng,\n",
        "                                        input=layer_input,\n",
        "                                        n_in=input_size,\n",
        "                                        n_out=hidden_layers_sizes[i],\n",
        "                                        activation=T.nnet.sigmoid)\n",
        "\n",
        "            # add the layer to our list of layers\n",
        "            self.sigmoid_layers.append(sigmoid_layer)\n",
        "\n",
        "            # its arguably a philosophical question...  but we are\n",
        "            # going to only declare that the parameters of the\n",
        "            # sigmoid_layers are parameters of the DBN. The visible\n",
        "            # biases in the RBM are parameters of those RBMs, but not\n",
        "            # of the DBN.\n",
        "            self.params.extend(sigmoid_layer.params)\n",
        "\n",
        "            # Construct an RBM that shared weights with this layer\n",
        "            rbm_layer = RBM(numpy_rng=numpy_rng,\n",
        "                            theano_rng=theano_rng,\n",
        "                            input=layer_input,\n",
        "                            n_visible=input_size,\n",
        "                            n_hidden=hidden_layers_sizes[i],\n",
        "                            W=sigmoid_layer.W,\n",
        "                            hbias=sigmoid_layer.b)\n",
        "            self.rbm_layers.append(rbm_layer)\n",
        "\n",
        "        # We now need to add a logistic layer on top of the MLP\n",
        "        self.logLayer = LogisticRegression(\n",
        "            input=self.sigmoid_layers[-1].output,\n",
        "            n_in=hidden_layers_sizes[-1],\n",
        "            n_out=n_outs)\n",
        "        self.params.extend(self.logLayer.params)\n",
        "\n",
        "        # compute the cost for second phase of training, defined as the\n",
        "        # negative log likelihood of the logistic regression (output) layer\n",
        "        self.finetune_cost = self.logLayer.negative_log_likelihood(self.y)\n",
        "\n",
        "        # compute the gradients with respect to the model parameters\n",
        "        # symbolic variable that points to the number of errors made on the\n",
        "        # minibatch given by self.x and self.y\n",
        "        self.errors = self.logLayer.errors(self.y)\n",
        "\n",
        "    def pretraining_functions(self, train_set_x, batch_size, k):\n",
        "        '''Generates a list of functions, for performing one step of\n",
        "        gradient descent at a given layer. The function will require\n",
        "        as input the minibatch index, and to train an RBM you just\n",
        "        need to iterate, calling the corresponding function on all\n",
        "        minibatch indexes.\n",
        "\n",
        "        :type train_set_x: theano.tensor.TensorType\n",
        "        :param train_set_x: Shared var. that contains all datapoints used\n",
        "                            for training the RBM\n",
        "        :type batch_size: int\n",
        "        :param batch_size: size of a [mini]batch\n",
        "        :param k: number of Gibbs steps to do in CD-k / PCD-k\n",
        "\n",
        "        '''\n",
        "\n",
        "        # index to a [mini]batch\n",
        "        index = T.lscalar('index')  # index to a minibatch\n",
        "        learning_rate = T.scalar('lr')  # learning rate to use\n",
        "\n",
        "        # begining of a batch, given `index`\n",
        "        batch_begin = index * batch_size\n",
        "        # ending of a batch given `index`\n",
        "        batch_end = batch_begin + batch_size\n",
        "\n",
        "        pretrain_fns = []\n",
        "        for rbm in self.rbm_layers:\n",
        "\n",
        "            # get the cost and the updates list\n",
        "            # using CD-k here (persisent=None) for training each RBM.\n",
        "            # TODO: change cost function to reconstruction error\n",
        "            cost, updates = rbm.get_cost_updates(learning_rate,\n",
        "                                                 persistent=None, k=k)\n",
        "\n",
        "            # compile the theano function\n",
        "            fn = theano.function(\n",
        "                inputs=[index, theano.In(learning_rate, value=0.1)],\n",
        "                outputs=cost,\n",
        "                updates=updates,\n",
        "                givens={\n",
        "                    self.x: train_set_x[batch_begin:batch_end]\n",
        "                }\n",
        "            )\n",
        "            # append `fn` to the list of functions\n",
        "            pretrain_fns.append(fn)\n",
        "\n",
        "        return pretrain_fns\n",
        "\n",
        "    def build_finetune_functions(self, datasets, batch_size, learning_rate):\n",
        "        '''Generates a function `train` that implements one step of\n",
        "        finetuning, a function `validate` that computes the error on a\n",
        "        batch from the validation set, and a function `test` that\n",
        "        computes the error on a batch from the testing set\n",
        "\n",
        "        :type datasets: list of pairs of theano.tensor.TensorType\n",
        "        :param datasets: It is a list that contain all the datasets;\n",
        "                        the has to contain three pairs, `train`,\n",
        "                        `valid`, `test` in this order, where each pair\n",
        "                        is formed of two Theano variables, one for the\n",
        "                        datapoints, the other for the labels\n",
        "        :type batch_size: int\n",
        "        :param batch_size: size of a minibatch\n",
        "        :type learning_rate: float\n",
        "        :param learning_rate: learning rate used during finetune stage\n",
        "\n",
        "        '''\n",
        "\n",
        "        (train_set_x, train_set_y) = datasets[0]\n",
        "        (valid_set_x, valid_set_y) = datasets[1]\n",
        "        (test_set_x, test_set_y) = datasets[2]\n",
        "\n",
        "        # compute number of minibatches for training, validation and testing\n",
        "        n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
        "        n_valid_batches //= batch_size\n",
        "        n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
        "        n_test_batches //= batch_size\n",
        "\n",
        "        index = T.lscalar('index')  # index to a [mini]batch\n",
        "\n",
        "        # compute the gradients with respect to the model parameters\n",
        "        gparams = T.grad(self.finetune_cost, self.params)\n",
        "\n",
        "        # compute list of fine-tuning updates\n",
        "        updates = []\n",
        "        for param, gparam in zip(self.params, gparams):\n",
        "            updates.append((param, param - gparam * learning_rate))\n",
        "\n",
        "        train_fn = theano.function(\n",
        "            inputs=[index],\n",
        "            outputs=self.finetune_cost,\n",
        "            updates=updates,\n",
        "            givens={\n",
        "                self.x: train_set_x[\n",
        "                    index * batch_size: (index + 1) * batch_size\n",
        "                ],\n",
        "                self.y: train_set_y[\n",
        "                    index * batch_size: (index + 1) * batch_size\n",
        "                ]\n",
        "            }\n",
        "        )\n",
        "\n",
        "        test_score_i = theano.function(\n",
        "            [index],\n",
        "            self.errors,\n",
        "            givens={\n",
        "                self.x: test_set_x[\n",
        "                    index * batch_size: (index + 1) * batch_size\n",
        "                ],\n",
        "                self.y: test_set_y[\n",
        "                    index * batch_size: (index + 1) * batch_size\n",
        "                ]\n",
        "            }\n",
        "        )\n",
        "\n",
        "        valid_score_i = theano.function(\n",
        "            [index],\n",
        "            self.errors,\n",
        "            givens={\n",
        "                self.x: valid_set_x[\n",
        "                    index * batch_size: (index + 1) * batch_size\n",
        "                ],\n",
        "                self.y: valid_set_y[\n",
        "                    index * batch_size: (index + 1) * batch_size\n",
        "                ]\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Create a function that scans the entire validation set\n",
        "        def valid_score():\n",
        "            return [valid_score_i(i) for i in range(n_valid_batches)]\n",
        "\n",
        "        # Create a function that scans the entire test set\n",
        "        def test_score():\n",
        "            return [test_score_i(i) for i in range(n_test_batches)]\n",
        "\n",
        "        return train_fn, valid_score, test_score\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm2I0cBBLrX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_DBN(finetune_lr=0.1, pretraining_epochs=100,\n",
        "             pretrain_lr=0.01, k=1, training_epochs=1000,\n",
        "             dataset='mnist.pkl.gz', batch_size=10):\n",
        "    \"\"\"\n",
        "    Demonstrates how to train and test a Deep Belief Network.\n",
        "\n",
        "    This is demonstrated on MNIST.\n",
        "\n",
        "    :type finetune_lr: float\n",
        "    :param finetune_lr: learning rate used in the finetune stage\n",
        "    :type pretraining_epochs: int\n",
        "    :param pretraining_epochs: number of epoch to do pretraining\n",
        "    :type pretrain_lr: float\n",
        "    :param pretrain_lr: learning rate to be used during pre-training\n",
        "    :type k: int\n",
        "    :param k: number of Gibbs steps in CD/PCD\n",
        "    :type training_epochs: int\n",
        "    :param training_epochs: maximal number of iterations ot run the optimizer\n",
        "    :type dataset: string\n",
        "    :param dataset: path the the pickled dataset\n",
        "    :type batch_size: int\n",
        "    :param batch_size: the size of a minibatch\n",
        "    \"\"\"\n",
        "\n",
        "    datasets = load_data(dataset)\n",
        "\n",
        "    train_set_x, train_set_y = datasets[0]\n",
        "    valid_set_x, valid_set_y = datasets[1]\n",
        "    test_set_x, test_set_y = datasets[2]\n",
        "\n",
        "    # compute number of minibatches for training, validation and testing\n",
        "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
        "\n",
        "    # numpy random generator\n",
        "    numpy_rng = numpy.random.RandomState(123)\n",
        "    print('... building the model')\n",
        "    # construct the Deep Belief Network\n",
        "    dbn = DBN(numpy_rng=numpy_rng, n_ins=28 * 28,\n",
        "              hidden_layers_sizes=[1000, 1000, 1000],\n",
        "              n_outs=10)\n",
        "\n",
        "    # start-snippet-2\n",
        "    #########################\n",
        "    # PRETRAINING THE MODEL #\n",
        "    #########################\n",
        "    print('... getting the pretraining functions')\n",
        "    pretraining_fns = dbn.pretraining_functions(train_set_x=train_set_x,\n",
        "                                                batch_size=batch_size,\n",
        "                                                k=k)\n",
        "\n",
        "    print('... pre-training the model')\n",
        "    start_time = timeit.default_timer()\n",
        "    # Pre-train layer-wise\n",
        "    for i in range(dbn.n_layers):\n",
        "        # go through pretraining epochs\n",
        "        for epoch in range(pretraining_epochs):\n",
        "            # go through the training set\n",
        "            c = []\n",
        "            for batch_index in range(n_train_batches):\n",
        "                c.append(pretraining_fns[i](index=batch_index,\n",
        "                                            lr=pretrain_lr))\n",
        "            print('Pre-training layer %i, epoch %d, cost ' % (i, epoch), end=' ')\n",
        "            print(numpy.mean(c, dtype='float64'))\n",
        "\n",
        "    end_time = timeit.default_timer()\n",
        "    # end-snippet-2\n",
        "    print('The pretraining code for file ' + os.path.split(__file__)[1] +\n",
        "          ' ran for %.2fm' % ((end_time - start_time) / 60.), file=sys.stderr)\n",
        "    ########################\n",
        "    # FINETUNING THE MODEL #\n",
        "    ########################\n",
        "\n",
        "    # get the training, validation and testing function for the model\n",
        "    print('... getting the finetuning functions')\n",
        "    train_fn, validate_model, test_model = dbn.build_finetune_functions(\n",
        "        datasets=datasets,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=finetune_lr\n",
        "    )\n",
        "\n",
        "    print('... finetuning the model')\n",
        "    # early-stopping parameters\n",
        "\n",
        "    # look as this many examples regardless\n",
        "    patience = 4 * n_train_batches\n",
        "\n",
        "    # wait this much longer when a new best is found\n",
        "    patience_increase = 2.\n",
        "\n",
        "    # a relative improvement of this much is considered significant\n",
        "    improvement_threshold = 0.995\n",
        "\n",
        "    # go through this many minibatches before checking the network on\n",
        "    # the validation set; in this case we check every epoch\n",
        "    validation_frequency = min(n_train_batches, patience / 2)\n",
        "\n",
        "    best_validation_loss = numpy.inf\n",
        "    test_score = 0.\n",
        "    start_time = timeit.default_timer()\n",
        "\n",
        "    done_looping = False\n",
        "    epoch = 0\n",
        "\n",
        "    while (epoch < training_epochs) and (not done_looping):\n",
        "        epoch = epoch + 1\n",
        "        for minibatch_index in range(n_train_batches):\n",
        "\n",
        "            train_fn(minibatch_index)\n",
        "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
        "\n",
        "            if (iter + 1) % validation_frequency == 0:\n",
        "\n",
        "                validation_losses = validate_model()\n",
        "                this_validation_loss = numpy.mean(validation_losses, dtype='float64')\n",
        "                print('epoch %i, minibatch %i/%i, validation error %f %%' % (\n",
        "                    epoch,\n",
        "                    minibatch_index + 1,\n",
        "                    n_train_batches,\n",
        "                    this_validation_loss * 100.\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                # if we got the best validation score until now\n",
        "                if this_validation_loss < best_validation_loss:\n",
        "\n",
        "                    # improve patience if loss improvement is good enough\n",
        "                    if (this_validation_loss < best_validation_loss *\n",
        "                            improvement_threshold):\n",
        "                        patience = max(patience, iter * patience_increase)\n",
        "\n",
        "                    # save best validation score and iteration number\n",
        "                    best_validation_loss = this_validation_loss\n",
        "                    best_iter = iter\n",
        "\n",
        "                    # test it on the test set\n",
        "                    test_losses = test_model()\n",
        "                    test_score = numpy.mean(test_losses, dtype='float64')\n",
        "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
        "                           'best model %f %%') %\n",
        "                          (epoch, minibatch_index + 1, n_train_batches,\n",
        "                          test_score * 100.))\n",
        "\n",
        "            if patience <= iter:\n",
        "                done_looping = True\n",
        "                break\n",
        "\n",
        "    end_time = timeit.default_timer()\n",
        "    print(('Optimization complete with best validation score of %f %%, '\n",
        "           'obtained at iteration %i, '\n",
        "           'with test performance %f %%'\n",
        "           ) % (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
        "    print('The fine tuning code for file ' + os.path.split(__file__)[1] +\n",
        "          ' ran for %.2fm' % ((end_time - start_time) / 60.), file=sys.stderr)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8xAUQtWLvOX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "652a8254-b6bd-4122-bf4e-523d850aa11b"
      },
      "source": [
        "test_DBN()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "... loading data\n",
            "... building the model\n",
            "... getting the pretraining functions\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n",
            "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "... pre-training the model\n",
            "Pre-training layer 0, epoch 0, cost  -98.53649282938044\n",
            "Pre-training layer 0, epoch 1, cost  -83.84455761672784\n",
            "Pre-training layer 0, epoch 2, cost  -80.69605640025738\n",
            "Pre-training layer 0, epoch 3, cost  -79.0383977598099\n",
            "Pre-training layer 0, epoch 4, cost  -77.9289304769324\n",
            "Pre-training layer 0, epoch 5, cost  -77.0886402508862\n",
            "Pre-training layer 0, epoch 6, cost  -76.40529544349154\n",
            "Pre-training layer 0, epoch 7, cost  -75.83012436144146\n",
            "Pre-training layer 0, epoch 8, cost  -75.34918889756972\n",
            "Pre-training layer 0, epoch 9, cost  -74.9350097407806\n",
            "Pre-training layer 0, epoch 10, cost  -74.59989711717591\n",
            "Pre-training layer 0, epoch 11, cost  -74.26493685621158\n",
            "Pre-training layer 0, epoch 12, cost  -73.92351015746843\n",
            "Pre-training layer 0, epoch 13, cost  -73.67669840156964\n",
            "Pre-training layer 0, epoch 14, cost  -73.43701242703939\n",
            "Pre-training layer 0, epoch 15, cost  -73.16817300402923\n",
            "Pre-training layer 0, epoch 16, cost  -72.95588844475819\n",
            "Pre-training layer 0, epoch 17, cost  -72.78851238145272\n",
            "Pre-training layer 0, epoch 18, cost  -72.60725132880395\n",
            "Pre-training layer 0, epoch 19, cost  -72.45116308191837\n",
            "Pre-training layer 0, epoch 20, cost  -72.26185533996681\n",
            "Pre-training layer 0, epoch 21, cost  -72.15908379149117\n",
            "Pre-training layer 0, epoch 22, cost  -72.01562005256376\n",
            "Pre-training layer 0, epoch 23, cost  -71.82556145074466\n",
            "Pre-training layer 0, epoch 24, cost  -71.7218750069688\n",
            "Pre-training layer 0, epoch 25, cost  -71.58140910457858\n",
            "Pre-training layer 0, epoch 26, cost  -71.447423228134\n",
            "Pre-training layer 0, epoch 27, cost  -71.3614574193402\n",
            "Pre-training layer 0, epoch 28, cost  -71.23344983896851\n",
            "Pre-training layer 0, epoch 29, cost  -71.16356968042511\n",
            "Pre-training layer 0, epoch 30, cost  -71.01690821607562\n",
            "Pre-training layer 0, epoch 31, cost  -70.95532571250448\n",
            "Pre-training layer 0, epoch 32, cost  -70.85535639678028\n",
            "Pre-training layer 0, epoch 33, cost  -70.78292380819943\n",
            "Pre-training layer 0, epoch 34, cost  -70.70454685149483\n",
            "Pre-training layer 0, epoch 35, cost  -70.61462779714483\n",
            "Pre-training layer 0, epoch 36, cost  -70.54902424582436\n",
            "Pre-training layer 0, epoch 37, cost  -70.4686320272244\n",
            "Pre-training layer 0, epoch 38, cost  -70.39379866735435\n",
            "Pre-training layer 0, epoch 39, cost  -70.32716292954115\n",
            "Pre-training layer 0, epoch 40, cost  -70.24438746878329\n",
            "Pre-training layer 0, epoch 41, cost  -70.16596861562022\n",
            "Pre-training layer 0, epoch 42, cost  -70.11247183905391\n",
            "Pre-training layer 0, epoch 43, cost  -70.06794513189821\n",
            "Pre-training layer 0, epoch 44, cost  -70.01492725439927\n",
            "Pre-training layer 0, epoch 45, cost  -69.98329181880209\n",
            "Pre-training layer 0, epoch 46, cost  -69.9048303120329\n",
            "Pre-training layer 0, epoch 47, cost  -69.84328393318313\n",
            "Pre-training layer 0, epoch 48, cost  -69.7827292325786\n",
            "Pre-training layer 0, epoch 49, cost  -69.73853034932407\n",
            "Pre-training layer 0, epoch 50, cost  -69.71331600266592\n",
            "Pre-training layer 0, epoch 51, cost  -69.69053075564955\n",
            "Pre-training layer 0, epoch 52, cost  -69.6353742678065\n",
            "Pre-training layer 0, epoch 53, cost  -69.56142868588479\n",
            "Pre-training layer 0, epoch 54, cost  -69.53323014477232\n",
            "Pre-training layer 0, epoch 55, cost  -69.48926180626724\n",
            "Pre-training layer 0, epoch 56, cost  -69.45778043063817\n",
            "Pre-training layer 0, epoch 57, cost  -69.44579925705617\n",
            "Pre-training layer 0, epoch 58, cost  -69.38749699909067\n",
            "Pre-training layer 0, epoch 59, cost  -69.36282712481982\n",
            "Pre-training layer 0, epoch 60, cost  -69.33754956970913\n",
            "Pre-training layer 0, epoch 61, cost  -69.30408155336912\n",
            "Pre-training layer 0, epoch 62, cost  -69.25979242410968\n",
            "Pre-training layer 0, epoch 63, cost  -69.23503281145014\n",
            "Pre-training layer 0, epoch 64, cost  -69.22550547146811\n",
            "Pre-training layer 0, epoch 65, cost  -69.20270764459495\n",
            "Pre-training layer 0, epoch 66, cost  -69.17273589792083\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}